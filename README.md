# ğŸ”¥ MultiMod-AI-Agent: Wildfire Detection Edition

A powerful multi-modal AI agent that fuses image and text analysis to identify and explain wildfire events using vision models and LLMs. Built to showcase advanced RAG pipelines and real-world AI applications on wildfire imagery.

---

## ğŸ“¸ Dataset

Uses the **Wildfire Detection Image Data** from [Kaggle](https://www.kaggle.com/datasets/brsdincer/wildfire-detection-image-data), which contains:

- ğŸ”¥ Wildfire images
- ğŸŒ² Non-wildfire images
- ğŸ“„ Optional accompanying documentation

---

## ğŸ’¡ Key Features

- ğŸ–¼ï¸ Analyze wildfire-related images using **CLIP** or **BLIP**
- ğŸ“„ Ingest related documents and reports (PDF, DOCX, TXT)
- ğŸ§  Use **RAG** (Retrieval-Augmented Generation) to answer natural language queries
- ğŸ’¬ Ask multi-modal questions like:  
  - â€œDoes this image show early or active wildfire stages?â€  
  - â€œWhat containment strategy fits the scene in this image?â€  
  - â€œCompare the document's advice with this visual situationâ€

---

## âš™ï¸ Tech Stack

| Component         | Technology                                      |
|------------------|--------------------------------------------------|
| Image Embedding  | CLIP / BLIP via HuggingFace Transformers         |
| Text Parsing     | PyMuPDF, python-docx, LangChain loaders          |
| Vector DB        | ChromaDB / PGVector / FAISS                      |
| LLM Reasoning    | LLaMA 3, GPT-4, or Mistral via LangChain         |
| UI               | Streamlit or FastAPI                             |
| Agent Framework  | LangChain (retrieval + reasoning + memory)       |

---

## ğŸ“ Project Structure
```bash
MultiMod-AI-Agent/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ main.py # Streamlit or FastAPI app
â”‚ â”œâ”€â”€ agent.py # Multi-modal reasoning logic
â”‚ â”œâ”€â”€ image_processor.py # Embeddings from wildfire images
â”‚ â”œâ”€â”€ doc_parser.py # PDF/DOCX/text chunking
â”‚ â”œâ”€â”€ vector_store.py # Store + retrieve chunks
â”‚ â””â”€â”€ utils.py
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ wildfire_images/ # Image dataset
â”‚ â””â”€â”€ sample_docs/ # Optional wildfire-related docs
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## âœ… Example Use Cases
Identify early fire warning signs from drone imagery

Suggest containment methods based on visual context

Cross-check document guidance with actual scenarios

Use embeddings for semantic search on fire scenes

## âš™ï¸ How It Works

1. **Upload** image(s) and document(s)
2. Image â†’ Caption & embed via BLIP/CLIP  
   Document â†’ Extracted, chunked, and embedded
3. Both sets go into a vector DB
4. RAG-based agent retrieves top chunks and calls LLM to generate a response
5. Build and Run - 
```yaml
docker build -t multimod-ai-agent .
docker run -p 8000:8000 multimod-ai-agent
```
---

## ğŸ“Œ Roadmap
 Vision + text fusion via RAG

 Add mic-input and audio queries

 Deploy via Hugging Face Spaces or Streamlit Cloud

 Train custom wildfire captioning model
 

## ğŸ“¦ Installation

```bash
git clone https://github.com/sukanyasaha007/MultiMod-AI-Agent.git
cd MultiMod-AI-Agent
pip install -r requirements.txt
python app/main.py
```

## ğŸ› ï¸ Features

ğŸ–¼ï¸ Visual grounding via CLIP/BLIP

ğŸ“„ Rich doc understanding (PDFs, DOCX)

ğŸ§  RAG + LLM for deep query responses

ğŸ“¤ Chat-like UX (via Streamlit or FastAPI)

ğŸ” Search & reasoning across modalities

## ğŸ“Œ Roadmap
 Multi-modal embedding & chunking

 RAG-based agent pipeline

 Audio-based querying (mic input)

 LangGraph for multi-step reasoning

 HuggingFace Space deployment

## ğŸ¤ Contributing
Pull requests and feature suggestions are welcome!
Letâ€™s build the future of multi-modal AI together ğŸ’¬ğŸ”—

## ğŸ“¬ Contact
Sukanya Saha
ğŸŒ LinkedIn
ğŸ’» GitHub

